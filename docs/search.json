[
  {
    "objectID": "Analysis.html",
    "href": "Analysis.html",
    "title": "Empirical analysis of order flow in selected Binance perpetual futures",
    "section": "",
    "text": "Introduction\nThe goal of this notebook is to follow some of the analysis of the paper “The price impact of order book events” by Cont, Kukanov, and Stoikov (2012). The later sections, however, do stray away from their analysis slightly. The paper is available here. Essentially, their paper studies order flow imbalance (OFI) and price impact with empirical analysis on randomly selected US stocks, finding OFI statistically significant in a linear model of instataneous price impact (admittedly, under an idealized order book model). Instead of US stock data, we will be using order book data from Binance (available here) for perpetual coin-denominated futures contracts for Bitcoin, Ethereum, and Solana during the period of January 2024 - May 2024. These three were chosen for being high volume traded cryptocurrencies on an overall scale, but analysis of lower volume coins could be of interest as well. Let us define some terms from the paper before we begin.\nDenote \\(\\delta\\) to be the tick size. Consider a time interval \\([t_{k-1}, t_k]\\), let \\(P^b_k\\) be the bid price at time \\(t_k\\) and \\(P^s_k\\) be the respective ask price. Let \\(P_k = \\frac{P_k^b + P_k^s}{2 \\delta}\\) be the tick-size normalized midprice and denote \\(\\Delta P_k = P_k - P_{k-1}\\) to be the midprice change. The tickers data from Binance contains the best bid and best ask price with their respective quantities at every tick. To calculate OFI, firstly consider the observations \\((P_n^b, q_n^b, P_n^s, q_n^s)\\) where \\(n\\) indexes each tick. Let \\[\ne_n = q_n^b \\text{ind}(P_n^b \\geq P_{n-1}^b) - q_{n-1}^b \\text{ind}(P_n^b \\leq P_{n-1}^b) - q_n^s \\text{ind}(P_n^s \\leq P_{n-1}^s) + q_{n-1}^s \\text{ind}(P_n^s \\geq P_{n-1}^s)\n\\] reprsent the signed contributions of order book events to supply/demand.\nNext, the paper considers the two uniform time grids \\(\\{T_0, ..., T_I\\}\\) and \\(\\{t_{0,0}, ..., t_{I,K}\\}\\) with \\(T_i - T_{i-1} = 30\\) minutes and \\(t_{k,i} - t_{k-1,i} = 10\\) seconds. Take \\(N()\\) to be a function such that \\(N(t_{k-1,i}) + 1\\) and \\(N(t_{k,i})\\) are the index of the first and last order book event in the interval \\([t_{k-1, i}, t_{k, i}]\\) Then within a large interval \\([T_{i-1}, T_i]\\), we have \\[\\Delta P_{k,i} = \\frac{P_{N(t_{k,i})}^b + P_{N(t_{k,i})}^s}{2 \\delta} - \\frac{P_{N(t_{k-1,i})}^b + P_{N(t_{k-1,i})}^s}{2 \\delta}\n\\] and \\[\\text{OFI}_{k,i} = \\sum_{n=N(t_{k-1,i})+1}^{N(t_{k,i})} e_n\n\\] The paper then considers the regression model\nThe paper also defines an estimator for depth in their stylized model on the interval \\([T_{i-1}, T_i]\\) as \\[\nD_i = \\frac{1}{2} \\left[\n\\frac{\n\\sum_{n=N(T_{i-1})+1}^{N(T_i)} \\left( q_n^b \\text{ind}\\left( P_n^b &lt; P_{n-1}^b \\right) + q_{n-1}^b \\text{ind}\\left( P_n^b &gt; P_{n-1}^b \\right) \\right)\n}{\n\\sum_{n=N(T_{i-1})+1}^{N(T_i)} \\text{ind}\\left( P_n^b \\neq P_{n-1}^b \\right)\n} +\n\\frac{\n\\sum_{n=N(T_{i-1})+1}^{N(T_i)} \\left( q_n^s \\text{ind}\\left( P_n^s &gt; P_{n-1}^s \\right) + q_{n-1}^s \\text{ind}\\left( P_n^s &lt; P_{n-1}^s \\right) \\right)\n}{\n\\sum_{n=N(T_{i-1})+1}^{N(T_i)} \\text{ind}\\left( P_n^s \\neq P_{n-1}^s \\right)\n}\n\\right]\n\\] I will not be considering the depth estimator in this notebook, but it could be of interest to consider in the future.\nThe model that the paper posits is \\[\n\\Delta P_{k,i} = \\beta_i \\text{OFI}_{k,i} + \\epsilon_{k,i}\n\\] where one can interpret it as a linear model of the instantaneous price impact of order book events arriving within the interval \\([t_{k-1}, t_k]\\). We will estimate it for the three perpectual contracts with OLS. There are some practical shortcomings with this model. But we will proceed with it for now. Later, we will consider adjustments to the model.\n\n\nData processing\nIn this section, I will walk through the process I use to prepare the data for statistical modeling. The data downloaded from Binance comes as a series of csv files, each containing the order book data for a single month. Here is a sample of the data:\n\n\nCode\nimport duckdb\nimport polars as pl\nimport glob\nimport os\n\n\n\n\nCode\npl.scan_csv(\"./data/ticker/BTCUSD_PERP-bookTicker-2024-01.csv\").head(5).collect()\n\n\n\n\nshape: (5, 7)\n\n\n\nupdate_id\nbest_bid_price\nbest_bid_qty\nbest_ask_price\nbest_ask_qty\ntransaction_time\nevent_time\n\n\ni64\nf64\nf64\nf64\nf64\ni64\ni64\n\n\n\n\n820182436406\n42305.4\n2672.0\n42305.5\n2584.0\n1704067200007\n1704067200013\n\n\n847746096711\n42773.0\n3493.0\n42773.1\n230.0\n1706664547018\n1706664547024\n\n\n820182436440\n42305.4\n3863.0\n42305.5\n2584.0\n1704067200010\n1704067200016\n\n\n847746096760\n42773.0\n3639.0\n42773.1\n230.0\n1706664547020\n1706664547027\n\n\n820182436494\n42305.4\n4739.0\n42305.5\n2584.0\n1704067200012\n1704067200024\n\n\n\n\n\n\n\nI will go through the data processing steps using BTCUSD_PERP. The same steps will be applied to ETHUSD_PERP and SOLUSD_PERP. Firstly, I will load the data into a DuckDB database since my RAM is limited. I think this might not be the most efficient format for timeseries, but it’s the easiest for me to setup for ingestion. Also, I’m not a SQL expert, so the following queries probably have optimizations. During the process, I will transform the transaction_time and event_time columns into timestamps and calculate the midprices. The distinction between transaction_time and event_time is that the former is when the data got recorded and the latter is when the data is pushed out from the server (see the discussion here or here). The difference between the two is on the order of milliseconds, so I suspect that it’s negligible for our purposes as we will be aggregating on a 10s scale.\n\n\nCode\nsymbol = \"BTCUSD_PERP\"\ntick_size = 0.1\ndata_dir = \"./data/ticker\"\nfiles = sorted(glob.glob(f\"{data_dir}/{symbol}*.csv\"))\ncon = duckdb.connect(f\"{data_dir}/{symbol}.db\")\ncols = f\"update_id, best_bid_price, best_bid_qty, best_ask_price, best_ask_qty, \\\n        ROUND((best_bid_price + best_ask_price) / (2 * {tick_size}), 1) AS mid_price, \\\n        EPOCH_MS(event_time) AS event_time, EPOCH_MS(transaction_time) AS transaction_time, \\\n        'BTCUSD_PERP' AS symbol\"\nquery = f\"\"\"\\\n        CREATE TABLE tick AS SELECT {cols} \n        FROM read_csv({files})\n        \"\"\"\ncon.execute(query)\n\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x10ba20d70&gt;\n\n\nLet’s take a quick look at what the data looks like now.\n\n\nCode\ncon.sql(\"SELECT * FROM tick LIMIT 5\")\n\n\n┌──────────────┬────────────────┬──────────────┬───┬──────────────────────┬──────────────────────┬─────────────┐\n│  update_id   │ best_bid_price │ best_bid_qty │ … │      event_time      │   transaction_time   │   symbol    │\n│    int64     │     double     │    double    │   │      timestamp       │      timestamp       │   varchar   │\n├──────────────┼────────────────┼──────────────┼───┼──────────────────────┼──────────────────────┼─────────────┤\n│ 820182436406 │        42305.4 │       2672.0 │ … │ 2024-01-01 00:00:0…  │ 2024-01-01 00:00:0…  │ BTCUSD_PERP │\n│ 847746096711 │        42773.0 │       3493.0 │ … │ 2024-01-31 01:29:0…  │ 2024-01-31 01:29:0…  │ BTCUSD_PERP │\n│ 820182436440 │        42305.4 │       3863.0 │ … │ 2024-01-01 00:00:0…  │ 2024-01-01 00:00:0…  │ BTCUSD_PERP │\n│ 847746096760 │        42773.0 │       3639.0 │ … │ 2024-01-31 01:29:0…  │ 2024-01-31 01:29:0…  │ BTCUSD_PERP │\n│ 820182436494 │        42305.4 │       4739.0 │ … │ 2024-01-01 00:00:0…  │ 2024-01-01 00:00:0…  │ BTCUSD_PERP │\n├──────────────┴────────────────┴──────────────┴───┴──────────────────────┴──────────────────────┴─────────────┤\n│ 5 rows                                                                                   9 columns (6 shown) │\n└──────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\nCode\ncon.sql(\"SELECT mid_price from tick LIMIT 10\")\n\n\n┌───────────┐\n│ mid_price │\n│  double   │\n├───────────┤\n│  423054.5 │\n│  427730.5 │\n│  423054.5 │\n│  427730.5 │\n│  423054.5 │\n│  427730.5 │\n│  423054.5 │\n│  427730.5 │\n│  423054.5 │\n│  427730.5 │\n├───────────┤\n│  10 rows  │\n└───────────┘\n\n\n\n\nCode\ncon.sql(\"SELECT count(update_id) FROM tick\")\n\n\n┌──────────────────┐\n│ count(update_id) │\n│      int64       │\n├──────────────────┤\n│        821486019 │\n└──────────────────┘\n\n\nNow I will calculate the \\(e_n\\) as defined above.\n\n\nCode\ncalculation = f\"\"\"\\\n                WITH cte AS (\n                SELECT \n                        best_bid_qty AS q_b_n,\n                        LAG(best_bid_qty, 1) OVER (ORDER BY transaction_time, update_id) AS q_b_n_minus_1,\n                        best_ask_qty AS q_s_n,\n                        LAG(best_ask_qty, 1) OVER (ORDER BY transaction_time, update_id) AS q_s_n_minus_1,\n                        best_bid_price AS P_b_n,\n                        LAG(best_bid_price, 1) OVER (ORDER BY transaction_time, update_id) AS P_b_n_minus_1,\n                        best_ask_price AS P_s_n,\n                        LAG(best_ask_price, 1) OVER (ORDER BY transaction_time, update_id) AS P_s_n_minus_1,\n                        mid_price,\n                        LAG (mid_price, 1) OVER (ORDER BY transaction_time, update_id) AS mid_price_lag,\n                        transaction_time,\n                        update_id,\n                        symbol\n                FROM tick\n                )\n                SELECT \n                (\n                        q_b_n * CASE WHEN P_b_n &gt;= P_b_n_minus_1 THEN 1 ELSE 0 END\n                        - q_b_n_minus_1 * CASE WHEN P_b_n &lt;= P_b_n_minus_1 THEN 1 ELSE 0 END\n                        - q_s_n * CASE WHEN P_s_n &lt;= P_s_n_minus_1 THEN 1 ELSE 0 END\n                        + q_s_n_minus_1 * CASE WHEN P_s_n &gt;= P_s_n_minus_1 THEN 1 ELSE 0 END\n                ) AS e_n,\n                mid_price - mid_price_lag AS mid_price_change,\n                transaction_time,\n                symbol\n                FROM \n                cte\n                ORDER BY transaction_time, update_id\n                \"\"\"\nquery = f\"\"\"\\\n        CREATE TABLE rolling AS\n        {calculation}\n        \"\"\"\ncon.execute(query)\n\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x10ba20d70&gt;\n\n\nThe resulting table looks like this:\n\n\nCode\ncon.sql(\"SELECT * FROM rolling LIMIT 5\")\n\n\n┌────────┬──────────────────┬─────────────────────────┬─────────────┐\n│  e_n   │ mid_price_change │    transaction_time     │   symbol    │\n│ double │      double      │        timestamp        │   varchar   │\n├────────┼──────────────────┼─────────────────────────┼─────────────┤\n│   NULL │             NULL │ 2024-01-01 00:00:00.007 │ BTCUSD_PERP │\n│ 1191.0 │              0.0 │ 2024-01-01 00:00:00.01  │ BTCUSD_PERP │\n│  876.0 │              0.0 │ 2024-01-01 00:00:00.012 │ BTCUSD_PERP │\n│  550.0 │              0.0 │ 2024-01-01 00:00:00.014 │ BTCUSD_PERP │\n│    5.0 │              0.0 │ 2024-01-01 00:00:00.015 │ BTCUSD_PERP │\n└────────┴──────────────────┴─────────────────────────┴─────────────┘\n\n\nFinally, let’s aggregate the data into 10s intervals and calculate the OFI and midprice changes. I’ll do \\((t_{k-1}, t_k]\\) so that the midprice change is calculated close to close.\n\n\nCode\nquery = f\"\"\"\\\n        CREATE TABLE agg_10s AS\n        WITH min_time_tbl AS (\n                SELECT date_trunc('year', MIN(transaction_time)) AS min_time\n                FROM rolling\n        )\n        SELECT \n        (\n                 min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n        ) AS time_bucket_end,\n        SUM(e_n) AS order_flow_imbalance,\n        SUM(mid_price_change) AS mid_price_change,\n        symbol,\n        COUNT(*) AS num_ticks\n        FROM rolling, min_time_tbl\n        GROUP BY symbol, min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n        ORDER BY time_bucket_end\n        \"\"\"\ncon.execute(query)\n\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x10ba20d70&gt;\n\n\nLet’s take a quick look at the data to make sure everything looks good.\n\n\nCode\ncon.sql(\"SELECT * FROM agg_10s LIMIT 5\")\n\n\n┌─────────────────────┬──────────────────────┬──────────────────┬─────────────┬───────────┐\n│   time_bucket_end   │ order_flow_imbalance │ mid_price_change │   symbol    │ num_ticks │\n│      timestamp      │        double        │      double      │   varchar   │   int64   │\n├─────────────────────┼──────────────────────┼──────────────────┼─────────────┼───────────┤\n│ 2024-01-01 00:00:10 │             -56352.0 │           -171.0 │ BTCUSD_PERP │       898 │\n│ 2024-01-01 00:00:20 │                777.0 │              8.0 │ BTCUSD_PERP │       692 │\n│ 2024-01-01 00:00:30 │              23537.0 │            112.0 │ BTCUSD_PERP │       624 │\n│ 2024-01-01 00:00:40 │              56473.0 │            133.0 │ BTCUSD_PERP │       861 │\n│ 2024-01-01 00:00:50 │               1594.0 │              0.0 │ BTCUSD_PERP │       364 │\n└─────────────────────┴──────────────────────┴──────────────────┴─────────────┴───────────┘\n\n\n\n\nCode\ncon.close()\n\n\nNow I’ll repeat the process for ETHUSD_PERP and SOLUSD_PERP.\n\n\nCode\nimport logging\n\n\ndef create_db(symbol, data_dir, tick_size):\n    logging.basicConfig(level=logging.INFO)\n    logging.info(f\"Creating database for {symbol}\")\n    files = sorted(glob.glob(f\"{data_dir}/{symbol}*.csv\"))\n    with duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n        cols = f\"update_id, best_bid_price, best_bid_qty, best_ask_price, best_ask_qty, \\\n                        ROUND((best_bid_price + best_ask_price) / (2 * {tick_size}), 1) AS mid_price, \\\n                        EPOCH_MS(event_time) AS event_time, EPOCH_MS(transaction_time) AS transaction_time, \\\n                        '{symbol}' AS symbol\"\n        query = f\"\"\"\\\n                        CREATE TABLE tick AS SELECT {cols} \n                        FROM read_csv({files})\n                        \"\"\"\n        con.execute(query)\n        logging.info(\"Created tick table\")\n        calculation = f\"\"\"\\\n                        WITH cte AS (\n                        SELECT \n                                best_bid_qty AS q_b_n,\n                                LAG(best_bid_qty, 1) OVER (ORDER BY transaction_time, update_id) AS q_b_n_minus_1,\n                                best_ask_qty AS q_s_n,\n                                LAG(best_ask_qty, 1) OVER (ORDER BY transaction_time, update_id) AS q_s_n_minus_1,\n                                best_bid_price AS P_b_n,\n                                LAG(best_bid_price, 1) OVER (ORDER BY transaction_time, update_id) AS P_b_n_minus_1,\n                                best_ask_price AS P_s_n,\n                                LAG(best_ask_price, 1) OVER (ORDER BY transaction_time, update_id) AS P_s_n_minus_1,\n                                mid_price,\n                                LAG (mid_price, 1) OVER (ORDER BY transaction_time, update_id) AS mid_price_lag,\n                                transaction_time,\n                                update_id,\n                                symbol\n                        FROM tick\n                        )\n                        SELECT \n                        (\n                                q_b_n * CASE WHEN P_b_n &gt;= P_b_n_minus_1 THEN 1 ELSE 0 END\n                                - q_b_n_minus_1 * CASE WHEN P_b_n &lt;= P_b_n_minus_1 THEN 1 ELSE 0 END\n                                - q_s_n * CASE WHEN P_s_n &lt;= P_s_n_minus_1 THEN 1 ELSE 0 END\n                                + q_s_n_minus_1 * CASE WHEN P_s_n &gt;= P_s_n_minus_1 THEN 1 ELSE 0 END\n                        ) AS e_n,\n                        mid_price - mid_price_lag AS mid_price_change,\n                        transaction_time,\n                        symbol\n                        FROM \n                        cte\n                        ORDER BY transaction_time, update_id\n                        \"\"\"\n        query = f\"\"\"\\\n                        CREATE TABLE rolling AS\n                        {calculation}\n                        \"\"\"\n        con.execute(query)\n        logging.info(\"Created rolling table\")\n        query = f\"\"\"\\\n                CREATE TABLE agg_10s AS\n                WITH min_time_tbl AS (\n                        SELECT date_trunc('year', MIN(transaction_time)) AS min_time\n                        FROM rolling\n                )\n                SELECT \n                (\n                        min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n                ) AS time_bucket_end,\n                SUM(e_n) AS order_flow_imbalance,\n                SUM(mid_price_change) AS mid_price_change,\n                symbol,\n                COUNT(*) AS num_ticks\n                FROM rolling, min_time_tbl\n                GROUP BY symbol, min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n                ORDER BY time_bucket_end\n                \"\"\"\n        con.execute(query)\n        logging.info(\"Created agg_10s table\")\n    logging.info(f\"Finished creating database for {symbol}\")\n\n\neth_tick_size = 0.01\nsol_tick_size = 0.001\ncreate_db(\"ETHUSD_PERP\", \"./data/ticker\", eth_tick_size)\ncreate_db(\"SOLUSD_PERP\", \"./data/ticker\", sol_tick_size)\n\n\nINFO:root:Creating database for ETHUSD_PERP\nINFO:root:Created tick table\nINFO:root:Created rolling table\nINFO:root:Created agg_10s table\nINFO:root:Finished creating database for ETHUSD_PERP\nINFO:root:Creating database for SOLUSD_PERP\nINFO:root:Created tick table\nINFO:root:Created rolling table\nINFO:root:Created agg_10s table\nINFO:root:Finished creating database for SOLUSD_PERP\n\n\nOverall, we have data that looks like this:\n\n\nCode\ndata_dir = \"./data/ticker\"\nsymbols = [\"BTCUSD_PERP\", \"ETHUSD_PERP\", \"SOLUSD_PERP\"]\nfor symbol in symbols:\n    with duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n        print(con.sql(\"SELECT COUNT(*) FROM agg_10s\"))\n        print(con.sql(\"SELECT * FROM agg_10s LIMIT 5 OFFSET 200000\"))\n\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      1313280 │\n└──────────────┘\n\n┌─────────────────────┬──────────────────────┬──────────────────┬─────────────┬───────────┐\n│   time_bucket_end   │ order_flow_imbalance │ mid_price_change │   symbol    │ num_ticks │\n│      timestamp      │        double        │      double      │   varchar   │   int64   │\n├─────────────────────┼──────────────────────┼──────────────────┼─────────────┼───────────┤\n│ 2024-01-24 03:33:30 │              -3388.0 │            -14.0 │ BTCUSD_PERP │       228 │\n│ 2024-01-24 03:33:40 │             -24267.0 │           -144.0 │ BTCUSD_PERP │       617 │\n│ 2024-01-24 03:33:50 │               4203.0 │             53.0 │ BTCUSD_PERP │       371 │\n│ 2024-01-24 03:34:00 │              61317.0 │            143.0 │ BTCUSD_PERP │       765 │\n│ 2024-01-24 03:34:10 │               9844.0 │            100.0 │ BTCUSD_PERP │       344 │\n└─────────────────────┴──────────────────────┴──────────────────┴─────────────┴───────────┘\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      1313280 │\n└──────────────┘\n\n┌─────────────────────┬──────────────────────┬──────────────────┬─────────────┬───────────┐\n│   time_bucket_end   │ order_flow_imbalance │ mid_price_change │   symbol    │ num_ticks │\n│      timestamp      │        double        │      double      │   varchar   │   int64   │\n├─────────────────────┼──────────────────────┼──────────────────┼─────────────┼───────────┤\n│ 2024-01-24 03:33:30 │              63829.0 │             31.0 │ ETHUSD_PERP │       333 │\n│ 2024-01-24 03:33:40 │            -194578.0 │           -116.0 │ ETHUSD_PERP │       633 │\n│ 2024-01-24 03:33:50 │             245886.0 │             25.0 │ ETHUSD_PERP │       456 │\n│ 2024-01-24 03:34:00 │             338321.0 │             93.0 │ ETHUSD_PERP │       494 │\n│ 2024-01-24 03:34:10 │             233834.0 │             56.0 │ ETHUSD_PERP │       495 │\n└─────────────────────┴──────────────────────┴──────────────────┴─────────────┴───────────┘\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      1313280 │\n└──────────────┘\n\n┌─────────────────────┬──────────────────────┬──────────────────┬─────────────┬───────────┐\n│   time_bucket_end   │ order_flow_imbalance │ mid_price_change │   symbol    │ num_ticks │\n│      timestamp      │        double        │      double      │   varchar   │   int64   │\n├─────────────────────┼──────────────────────┼──────────────────┼─────────────┼───────────┤\n│ 2024-01-24 03:33:30 │               1083.0 │              3.0 │ SOLUSD_PERP │        79 │\n│ 2024-01-24 03:33:40 │              -6630.0 │            -63.0 │ SOLUSD_PERP │       140 │\n│ 2024-01-24 03:33:50 │               5584.0 │             41.5 │ SOLUSD_PERP │        90 │\n│ 2024-01-24 03:34:00 │               9863.0 │             70.5 │ SOLUSD_PERP │       137 │\n│ 2024-01-24 03:34:10 │               1103.0 │            -18.0 │ SOLUSD_PERP │       115 │\n└─────────────────────┴──────────────────────┴──────────────────┴─────────────┴───────────┘\n\n\n\nThe activity in SOL seems lower than the other two.\n\n\nStatistical modeling\nRecall, the model we are interested in is \\[\n\\Delta P_{k,i} = \\beta_i \\text{OFI}_{k,i} + \\epsilon_{k,i}\n\\] Through OLS, we will estimate \\[\n\\Delta P_{k,i} = \\hat{\\alpha}_i + \\hat{\\beta}_i \\text{OFI}_{k,i} + \\hat{\\epsilon}_{k,i}\n\\] for each of the three contracts. The aggregated data is small enough to fit in memory, so I will switch to Pandas and statsmodels for this part. When fitting the models, the paper uses HAC standard errors to account for autocorrelation in the residuals. I will do the same.\n\n\nCode\nimport statsmodels.formula.api as smf\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom diagnostics import LinearRegDiagnostic\n\n\nLet’s first make a plot for a random interval for BTCUSD_PERP to see if there is any visual correlation between OFI and midprice change.\n\n\nCode\nnp.random.seed(42)\ninstance = np.random.randint(0, 1313280 // 180)\nwith duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n    df = con.sql(\"SELECT * FROM agg_10s\").to_df()\n    df = df.iloc[instance * 180 : (instance + 1) * 180]\n    model = smf.ols(\"mid_price_change ~ order_flow_imbalance\", data=df)\n    res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(180**0.25)})\n    sns.scatterplot(data=df, x=\"order_flow_imbalance\", y=\"mid_price_change\")\n    sns.lineplot(x=df.order_flow_imbalance, y=res.fittedvalues, color=\"red\")\n\n\n\n\n\n\n\n\n\nIt does seem like there is a linear relationship between the two, for this interval at least. I’ll now fit the model for all the 30 minute intervals for each of the three contracts.\n\n\nCode\nclass Results:\n    def __init__(self):\n        self.alphas = []\n        self.betas = []\n        self.alphas_p = []\n        self.betas_p = []\n        self.r2 = []\n\n    def summarize(self):\n        signif_alphas = np.sum(np.array(self.alphas_p) &lt; 0.05) / len(self.alphas_p)\n        signif_betas = np.sum(np.array(self.betas_p) &lt; 0.05) / len(self.betas_p)\n        avg_r2 = np.mean(self.r2)\n        return pd.DataFrame(\n            {\n                \"signif_alphas\": [signif_alphas],\n                \"signif_betas\": [signif_betas],\n                \"avg_r2\": [avg_r2],\n            }\n        )\n\n\ndef global_summary(*results):\n    dfs = [result.summarize() for result in results]\n    concacted = pd.concat(dfs)\n    return concacted.mean()\n\n\ndef estimate_regressions(symbol, data_dir, every=180):\n    con = duckdb.connect(f\"{data_dir}/{symbol}.db\")\n    query = f\"\"\"\\\n            SELECT * FROM agg_10s\n            \"\"\"\n    df = con.execute(query).df()\n    con.close()\n    results = Results()\n    for i in range(0, len(df), every):\n        model = smf.ols(\n            \"mid_price_change ~ order_flow_imbalance\", data=df.iloc[i : i + every]\n        )\n        res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(every**0.25)})\n        results.alphas.append(res.params[\"Intercept\"])\n        results.betas.append(res.params[\"order_flow_imbalance\"])\n        results.alphas_p.append(res.pvalues[\"Intercept\"])\n        results.betas_p.append(res.pvalues[\"order_flow_imbalance\"])\n        results.r2.append(res.rsquared)\n    return results\n\n\nbtc_results = estimate_regressions(\"BTCUSD_PERP\", \"./data/ticker\")\neth_results = estimate_regressions(\"ETHUSD_PERP\", \"./data/ticker\")\nsol_results = estimate_regressions(\"SOLUSD_PERP\", \"./data/ticker\")\n\n\n\n\nCode\nbtc_results.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.204221\n1.0\n0.697402\n\n\n\n\n\n\n\n\n\n\nCode\neth_results.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.310444\n0.999863\n0.706825\n\n\n\n\n\n\n\n\n\n\nCode\nsol_results.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.263569\n0.992325\n0.542729\n\n\n\n\n\n\n\n\n\n\nCode\nglobal_summary(btc_results, eth_results, sol_results)\n\n\nsignif_alphas    0.259412\nsignif_betas     0.997396\navg_r2           0.648985\ndtype: float64\n\n\n\n\nCode\nfrom statsmodels.stats.multitest import multipletests\n\nreject, _, _, _ = multipletests(\n    np.array(sol_results.betas_p), alpha=0.05, method=\"fdr_bh\"\n)\nnp.average(reject)\n\n\nnp.float64(0.9923245614035088)\n\n\nOverall, we have similar results as the paper. \\(\\hat{\\beta}\\) is significant (when p-value is &lt; .05) in &gt;99% of the cases. The \\(R^2\\) values average to around 65%. So a linear model of instantaneous price-impact does quite well. Let’s try using 1 hour sized intervals instead of 30 minutes.\n\n\nCode\nbtc_results_hr = estimate_regressions(\"BTCUSD_PERP\", \"./data/ticker\", every=360)\neth_results_hr = estimate_regressions(\"ETHUSD_PERP\", \"./data/ticker\", every=360)\nsol_results_hr = estimate_regressions(\"SOLUSD_PERP\", \"./data/ticker\", every=360)\n\n\n\n\nCode\nbtc_results_hr.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.238761\n1.0\n0.680708\n\n\n\n\n\n\n\n\n\n\nCode\neth_results_hr.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.33909\n1.0\n0.690052\n\n\n\n\n\n\n\n\n\n\nCode\nsol_results_hr.summarize()\n\n\n\n\n\n\n\n\n\n\nsignif_alphas\nsignif_betas\navg_r2\n\n\n\n\n0\n0.26261\n0.996436\n0.520113\n\n\n\n\n\n\n\n\n\n\nCode\nglobal_summary(btc_results_hr, eth_results_hr, sol_results_hr)\n\n\nsignif_alphas    0.280154\nsignif_betas     0.998812\navg_r2           0.630291\ndtype: float64\n\n\nWe have similar findings for the hourly intervals too, indicating that the model could be robust over different time scales. I will not try different aggregations on the smaller time scale (e.g. 5s or 1s instead of 10s) as it just takes too long to run on my machine.\nIs this model useful in a practical setting to make trades? Probably not. For one, it estimates the price impact over a time interval using data from that very same interval. The original paper proposes a way to use this method of analysis as a monitoring mechanism for adverse selection. I will not go into that here, but it could be of interest to explore in the future. Now, if we try to predict the change in the next interval, performance drops significantly.\n\n\nCode\ndef estimate_regressions_fwd(symbol, data_dir, every=180):\n    con = duckdb.connect(f\"{data_dir}/{symbol}.db\")\n    query = f\"\"\"\\\n            SELECT * FROM agg_10s\n            \"\"\"\n    df = con.execute(query).df()\n    con.close()\n    df[\"mid_price_change_next\"] = df[\"mid_price_change\"].shift(-1)\n    results = Results()\n    for i in range(0, len(df), every):\n        model = smf.ols(\n            \"mid_price_change_next ~ order_flow_imbalance\",\n            data=df.iloc[i : i + every],\n            missing=\"drop\",\n        )\n        res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(every**0.25)})\n        results.alphas.append(res.params[\"Intercept\"])\n        results.betas.append(res.params[\"order_flow_imbalance\"])\n        results.alphas_p.append(res.pvalues[\"Intercept\"])\n        results.betas_p.append(res.pvalues[\"order_flow_imbalance\"])\n        results.r2.append(res.rsquared)\n    return results\n\n\nbtc_results_fwd = estimate_regressions_fwd(\"BTCUSD_PERP\", \"./data/ticker\")\neth_results_fwd = estimate_regressions_fwd(\"ETHUSD_PERP\", \"./data/ticker\")\nsol_results_fwd = estimate_regressions_fwd(\"SOLUSD_PERP\", \"./data/ticker\")\nprint(btc_results_fwd.summarize())\nprint(eth_results_fwd.summarize())\nprint(sol_results_fwd.summarize())\n\n\n   signif_alphas  signif_betas    avg_r2\n0       0.032895      0.412418  0.026467\n   signif_alphas  signif_betas    avg_r2\n0       0.035773      0.292626  0.017402\n   signif_alphas  signif_betas    avg_r2\n0       0.038377      0.124863  0.008959\n\n\nAnother concern is we are trying to estimate changes in price levels instead of returns, which means it must be estimated on a per asset basis. I’m curious if there’s any generalization across the contracts. Going forward, I will try to implement a more practical model.\n\n\nCan we use OFI as a signal?\nTo start, I want to have returns over the 10s intervals. I also want to do standardizations on the OFI. I will define returns as the change in the final midprice of one interval to the next. But everything done can be repeated for the bid/ask price as well.\n\n\nCode\nsymbols = [\"BTCUSD_PERP\", \"ETHUSD_PERP\", \"SOLUSD_PERP\"]\ndata_dir = \"./data/ticker\"\nfor symbol in symbols:\n    with duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n        con.execute(\n            f\"\"\"\n                    CREATE TABLE close_10s AS\n                    WITH base_data AS (\n                        SELECT \n                            transaction_time,\n                            mid_price,\n                            symbol,\n                            date_trunc('year', MIN(transaction_time) OVER ()) AS min_time\n                        FROM tick\n                    )\n                    SELECT\n                        MAX_BY(mid_price, transaction_time) AS close_price,\n                        symbol,\n                        (min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT) AS time_bucket_end\n                    FROM base_data\n                    GROUP BY symbol, min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n                    ORDER BY time_bucket_end\n                    \"\"\"\n        )\n\n\n\n\nCode\nsymbols = [\"BTCUSD_PERP\", \"ETHUSD_PERP\", \"SOLUSD_PERP\"]\ndata_dir = \"./data/ticker\"\nfor symbol in symbols:\n    with duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n        con.execute(\n            f\"\"\"\n                    CREATE TABLE open_10s AS\n                    WITH base_data AS (\n                        SELECT \n                            transaction_time,\n                            mid_price,\n                            symbol,\n                            date_trunc('year', MIN(transaction_time) OVER ()) AS min_time\n                        FROM tick\n                    )\n                    SELECT\n                        MIN_BY(mid_price, transaction_time) AS open_price,\n                        symbol,\n                        (min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT) AS time_bucket_end\n                    FROM base_data\n                    GROUP BY symbol, min_time + INTERVAL '10 seconds' * (FLOOR(DATE_PART('epoch', transaction_time - min_time) / 10) + 1)::INT\n                    ORDER BY time_bucket_end\n                    \"\"\"\n        )\n\n\n\n\nCode\ndef collect_data():\n    ofi_dfs = []\n    close_dfs = []\n    for symbol in symbols:\n        with duckdb.connect(f\"{data_dir}/{symbol}.db\") as con:\n            ofi_df = con.sql(\"SELECT * FROM agg_10s\").to_df()\n            close_df = con.sql(\"SELECT * FROM close_10s\").to_df()\n            ofi_dfs.append(ofi_df)\n            close_dfs.append(close_df)\n    return ofi_dfs, close_dfs\n\n\nofi_dfs, close_dfs = collect_data()\n\n\n\n\nCode\nfor df1, df2 in zip(ofi_dfs, close_dfs):\n    df1[\"close_price\"] = df2[\"close_price\"]\n    df1[\"returns\"] = df2[\"close_price\"].pct_change()\n    del df2\ndel close_dfs\n\n\nFor standardization of OFI, I will use a rolling window of 1 minutes to compute z-scores. This is to account for the different levels of activity in the three contracts. I will then use the z-scores in the regression model.\n\n\nCode\nfor df in ofi_dfs:\n    df[\"returns_next\"] = df[\"returns\"].shift(-1)\n    df[\"ofi_rolling_mean\"] = df[\"order_flow_imbalance\"].rolling(6).mean()\n    df[\"ofi_rolling_std\"] = df[\"order_flow_imbalance\"].rolling(6).std()\n    df[\"ofi_z\"] = (df[\"order_flow_imbalance\"] - df[\"ofi_rolling_mean\"]) / df[\n        \"ofi_rolling_std\"\n    ]\n\n\n\n\nCode\nfor df in ofi_dfs:\n    sns.lineplot(data=df[1000:2000], x=\"time_bucket_end\", y=\"ofi_z\", alpha=0.7)\n\n\n\n\n\n\n\n\n\nThe standardization seems to put the OFI in the same range for all three contracts. A quick visualization for a thirty minute period doesn’t show any obvious relationship between OFI and next 10s returns.\n\n\nCode\nfor df in ofi_dfs:\n    sns.scatterplot(data=df.iloc[4000:4180], x=\"ofi_z\", y=\"returns_next\")\n\n\n\n\n\n\n\n\n\nBut I will repeat the procedure from above anyways.\n\n\nCode\nclass Results:\n    def __init__(self):\n        self.alphas = []\n        self.betas = []\n        self.alphas_p = []\n        self.betas_p = []\n        self.betas_t = []\n        self.r2 = []\n\n    def summarize(self):\n        signif_alphas = np.sum(np.array(self.alphas_p) &lt; 0.05) / len(self.alphas_p)\n        signif_betas = np.sum(np.array(self.betas_p) &lt; 0.05) / len(self.betas_p)\n        avg_r2 = np.mean(self.r2)\n        return pd.DataFrame(\n            {\n                \"signif_alphas\": [signif_alphas],\n                \"signif_betas\": [signif_betas],\n                \"avg_t\": [np.mean(self.betas_t)],\n                \"avg_r2\": [avg_r2],\n            }\n        )\n\n\ndef global_summary(*results):\n    dfs = [result.summarize() for result in results]\n    concacted = pd.concat(dfs)\n    return concacted.mean()\n\n\ndef estimate_regressions(df, every=180):\n    results = Results()\n    df = df.dropna()\n    for i in range(0, len(df), every):\n        model = smf.ols(\"returns_next ~ ofi_z\", data=df.iloc[i : i + every])\n        res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(every**0.25)})\n        results.alphas.append(res.params[\"Intercept\"])\n        results.betas.append(res.params[\"ofi_z\"])\n        results.alphas_p.append(res.pvalues[\"Intercept\"])\n        results.betas_p.append(res.pvalues[\"ofi_z\"])\n        results.betas_t.append(res.tvalues[\"ofi_z\"])\n        results.r2.append(res.rsquared)\n    return results\n\n\nbtc_results = estimate_regressions(ofi_dfs[0])\neth_results = estimate_regressions(ofi_dfs[1])\nsol_results = estimate_regressions(ofi_dfs[2])\n\n\n\n\nCode\nprint(btc_results.summarize())\nprint(eth_results.summarize())\nprint(sol_results.summarize())\n\n\n   signif_alphas  signif_betas    avg_t    avg_r2\n0       0.040159       0.39693  1.64369  0.023374\n   signif_alphas  signif_betas     avg_t    avg_r2\n0       0.037144      0.294819  1.273421  0.016652\n   signif_alphas  signif_betas     avg_t    avg_r2\n0        0.03687      0.111431  0.472614  0.008378\n\n\nIt’s about what we expect. Let’s also run a regression over all the data with all the contracts pooled together.\n\n\nCode\ndef estimate_regressions_pooled(ofi_dfs):\n    df = pd.concat(ofi_dfs)\n    df = df.dropna()\n    model = smf.ols(\"returns_next ~ ofi_z\", data=df)\n    res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": int(len(df) ** 0.25)})\n    return res\n\n\npooled_results = estimate_regressions_pooled(ofi_dfs)\npooled_results.summary()\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nreturns_next\nR-squared:\n0.002\n\n\nModel:\nOLS\nAdj. R-squared:\n0.002\n\n\nMethod:\nLeast Squares\nF-statistic:\n6631.\n\n\nDate:\nSat, 29 Jun 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n23:31:47\nLog-Likelihood:\n2.4757e+07\n\n\nNo. Observations:\n3939822\nAIC:\n-4.951e+07\n\n\nDf Residuals:\n3939820\nBIC:\n-4.951e+07\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n4.952e-07\n2.26e-07\n2.187\n0.029\n5.14e-08\n9.39e-07\n\n\nofi_z\n2.202e-05\n2.7e-07\n81.431\n0.000\n2.15e-05\n2.25e-05\n\n\n\n\n\n\nOmnibus:\n2017399.515\nDurbin-Watson:\n1.985\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n13986223540.388\n\n\nSkew:\n-0.576\nProb(JB):\n0.00\n\n\nKurtosis:\n294.887\nCond. No.\n1.08\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 44 lags and without small sample correction\n\n\n\nWe get a significant coefficient, but it’s very bad at explaining the variance. So this normalized OFI does have useful information; it’s a question of how to use it. Going back to the regressions for next 10s returns partitioned over the thirty minute intervals, perhaps a hypothesis to test is if the estimated \\(\\hat{\\beta}\\) is significant in a thirty minute interval (by t-stat) then it will relatively do worse in the next thirty minute interval.\n\n\nCode\ndef get_times(every=180):\n    df = ofi_dfs[0]\n    times = []\n    for i in range(0, len(df), every):\n        times.append(df.iloc[i].time_bucket_end)\n    return times\n\n\ntimes = get_times()\nassert len(times) == len(btc_results.betas_t)\nassert len(times) == len(eth_results.betas_t)\nassert len(times) == len(sol_results.betas_t)\n\nbtc_t_arr = np.array(btc_results.betas_t)\n\neth_t_arr = np.array(eth_results.betas_t)\n\nsol_t_arr = np.array(sol_results.betas_t)\n\nt_arr = np.column_stack((btc_t_arr, eth_t_arr, sol_t_arr))\nsignal = -t_arr / np.abs(t_arr).sum(axis=1)[:, None]\nsignal[np.isnan(signal)] = 0\nsignal = signal[:-1]\n\ntradeable_times = [t + pd.Timedelta(seconds=20) for t in times[1:]]\n\nreturns_arr = np.column_stack(\n    [df.loc[df.time_bucket_end.isin(tradeable_times)][\"returns\"] for df in ofi_dfs]\n)\n\npd.DataFrame(\n    {\n        \"cumulative_returns\": (signal * returns_arr).sum(axis=1).cumsum(),\n        \"time\": tradeable_times,\n    }\n).plot(x=\"time\", y=\"cumulative_returns\")\n\n\n\n\n\n\n\n\n\nWell, that might just be noise. Maybe, life is simpler and we can use the OFI z-scores directly to weight a portfolio. The justification here is that the paper found that there is autocorrelation in OFI.\n\n\nCode\nofi_arr = np.column_stack([df.dropna()[\"ofi_z\"] for df in ofi_dfs])\nret_next_arr = np.column_stack([df.dropna()[\"returns_next\"] for df in ofi_dfs])\nofi_signal = ofi_arr / np.abs(ofi_arr).sum(axis=1)[:, None]\n\npd.DataFrame(\n    {\n        \"cumulative_returns\": (ofi_signal * ret_next_arr).sum(axis=1).cumsum(),\n        \"time\": ofi_dfs[0].dropna()[\"time_bucket_end\"],\n    }\n).plot(x=\"time\", y=\"cumulative_returns\")\n\n\n\n\n\n\n\n\n\nInteresting, however this doesn’t account for any trading costs, bid-ask spread, and so many other things. Perhaps something similar works on a longer time scale and across many more assets. I will end here for now, however.\n\n\nConclusion\nI guess the main takeaway is contemporaneous price change can be linearly modeled with OFI even for cryptocurrency perpetual contracts. This isn’t anything new or surprising (e.g. here or here), but sometimes that’s just how science works.\nAn avenue to explore could be thinking of Binance as the main reference for prices, then perhaps the OFI on Binance could be used to predict the price changes on other exchanges."
  }
]